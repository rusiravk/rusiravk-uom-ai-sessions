{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f04444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1155d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        price  area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
      "0    13300000  7420         4          2        3      yes        no       no   \n",
      "1    12250000  8960         4          4        4      yes        no       no   \n",
      "2    12250000  9960         3          2        2      yes        no      yes   \n",
      "3    12215000  7500         4          2        2      yes        no      yes   \n",
      "4    11410000  7420         4          1        2      yes       yes      yes   \n",
      "..        ...   ...       ...        ...      ...      ...       ...      ...   \n",
      "540   1820000  3000         2          1        1      yes        no      yes   \n",
      "541   1767150  2400         3          1        1       no        no       no   \n",
      "542   1750000  3620         2          1        1      yes        no       no   \n",
      "543   1750000  2910         3          1        1       no        no       no   \n",
      "544   1750000  3850         3          1        2      yes        no       no   \n",
      "\n",
      "    hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
      "0                no             yes        2      yes        furnished  \n",
      "1                no             yes        3       no        furnished  \n",
      "2                no              no        2      yes   semi-furnished  \n",
      "3                no             yes        3      yes        furnished  \n",
      "4                no             yes        2       no        furnished  \n",
      "..              ...             ...      ...      ...              ...  \n",
      "540              no              no        2       no      unfurnished  \n",
      "541              no              no        0       no   semi-furnished  \n",
      "542              no              no        0       no      unfurnished  \n",
      "543              no              no        0       no        furnished  \n",
      "544              no              no        0       no      unfurnished  \n",
      "\n",
      "[545 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Housing.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0fd13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = data['price'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d41b7739",
   "metadata": {},
   "outputs": [],
   "source": [
    "facility_data = data.iloc[:, 1:].to_numpy()\n",
    "facility_data = np.where(facility_data == 'yes', 1, facility_data)\n",
    "facility_data = np.where(facility_data == 'no', 0, facility_data)\n",
    "facility_data = np.where(facility_data == 'furnished', 2, facility_data)\n",
    "facility_data = np.where(facility_data == 'semi-furnished', 1, facility_data)\n",
    "facility_data = np.where(facility_data == 'unfurnished', 0, facility_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5703a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(facility_data, price, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d4d7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000\n",
    "rate = 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "789389cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1771: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 12909518078209.49804688\n",
      "Iteration 2, loss = 12896185773859.78906250\n",
      "Iteration 3, loss = 12862581846192.66992188\n",
      "Iteration 4, loss = 12800573067595.00781250\n",
      "Iteration 5, loss = 12704713210746.63867188\n",
      "Iteration 6, loss = 12570287699637.46289062\n",
      "Iteration 7, loss = 12404244422428.96093750\n",
      "Iteration 8, loss = 12196470009202.55859375\n",
      "Iteration 9, loss = 11945728431771.09179688\n",
      "Iteration 10, loss = 11669370662781.92578125\n",
      "Iteration 11, loss = 11362186715247.85351562\n",
      "Iteration 12, loss = 11015528567864.31445312\n",
      "Iteration 13, loss = 10645396858717.26562500\n",
      "Iteration 14, loss = 10259769449600.29882812\n",
      "Iteration 15, loss = 9853425696012.18164062\n",
      "Iteration 16, loss = 9439261082821.93359375\n",
      "Iteration 17, loss = 9018964774674.80859375\n",
      "Iteration 18, loss = 8579543087005.77050781\n",
      "Iteration 19, loss = 8149461393343.62109375\n",
      "Iteration 20, loss = 7723328650597.82519531\n",
      "Iteration 21, loss = 7321666734496.26367188\n",
      "Iteration 22, loss = 6905720566684.93847656\n",
      "Iteration 23, loss = 6521569478378.97265625\n",
      "Iteration 24, loss = 6142892829198.23242188\n",
      "Iteration 25, loss = 5792146064314.04101562\n",
      "Iteration 26, loss = 5462402281519.65234375\n",
      "Iteration 27, loss = 5148314750509.22656250\n",
      "Iteration 28, loss = 4858090153643.27539062\n",
      "Iteration 29, loss = 4585889265074.71386719\n",
      "Iteration 30, loss = 4338085089873.89160156\n",
      "Iteration 31, loss = 4112538092812.31054688\n",
      "Iteration 32, loss = 3894723711702.80126953\n",
      "Iteration 33, loss = 3709203354322.50537109\n",
      "Iteration 34, loss = 3525425349101.88769531\n",
      "Iteration 35, loss = 3366534412016.10742188\n",
      "Iteration 36, loss = 3212104081859.29003906\n",
      "Iteration 37, loss = 3070639985660.96777344\n",
      "Iteration 38, loss = 2942975973353.76953125\n",
      "Iteration 39, loss = 2819233708242.98388672\n",
      "Iteration 40, loss = 2702680501148.73193359\n",
      "Iteration 41, loss = 2594079445238.29443359\n",
      "Iteration 42, loss = 2491883814196.60693359\n",
      "Iteration 43, loss = 2392435213566.44091797\n",
      "Iteration 44, loss = 2300938009784.55175781\n",
      "Iteration 45, loss = 2211302074768.10546875\n",
      "Iteration 46, loss = 2127318559201.13256836\n",
      "Iteration 47, loss = 2047397457298.27587891\n",
      "Iteration 48, loss = 1968747015456.51513672\n",
      "Iteration 49, loss = 1898792145868.56494141\n",
      "Iteration 50, loss = 1828547729180.88256836\n",
      "Iteration 51, loss = 1764837407918.88159180\n",
      "Iteration 52, loss = 1701527256280.38354492\n",
      "Iteration 53, loss = 1643679682032.93969727\n",
      "Iteration 54, loss = 1588275876983.13281250\n",
      "Iteration 55, loss = 1537379674997.41235352\n",
      "Iteration 56, loss = 1487111935454.46191406\n",
      "Iteration 57, loss = 1441226079683.47192383\n",
      "Iteration 58, loss = 1397617637123.43627930\n",
      "Iteration 59, loss = 1358733210131.36108398\n",
      "Iteration 60, loss = 1321173930486.60278320\n",
      "Iteration 61, loss = 1285213964581.19287109\n",
      "Iteration 62, loss = 1251974215180.67138672\n",
      "Iteration 63, loss = 1221507956984.29028320\n",
      "Iteration 64, loss = 1192814535234.10864258\n",
      "Iteration 65, loss = 1166177590316.74902344\n",
      "Iteration 66, loss = 1141726059752.89306641\n",
      "Iteration 67, loss = 1118145205520.85693359\n",
      "Iteration 68, loss = 1095122737814.30981445\n",
      "Iteration 69, loss = 1074844465773.48937988\n",
      "Iteration 70, loss = 1055437593415.90454102\n",
      "Iteration 71, loss = 1036991541017.22534180\n",
      "Iteration 72, loss = 1019181874613.83630371\n",
      "Iteration 73, loss = 1003044724501.10070801\n",
      "Iteration 74, loss = 987733626751.77001953\n",
      "Iteration 75, loss = 971731172334.72033691\n",
      "Iteration 76, loss = 957616469438.67663574\n",
      "Iteration 77, loss = 945066349523.39611816\n",
      "Iteration 78, loss = 930537195862.81408691\n",
      "Iteration 79, loss = 918316350935.41174316\n",
      "Iteration 80, loss = 906523991990.70910645\n",
      "Iteration 81, loss = 894982389872.68322754\n",
      "Iteration 82, loss = 882962402810.71752930\n",
      "Iteration 83, loss = 872743399117.63830566\n",
      "Iteration 84, loss = 862195502597.21240234\n",
      "Iteration 85, loss = 852151704415.67846680\n",
      "Iteration 86, loss = 842471386358.22412109\n",
      "Iteration 87, loss = 832478252388.71057129\n",
      "Iteration 88, loss = 823266786772.80822754\n",
      "Iteration 89, loss = 814609158441.59033203\n",
      "Iteration 90, loss = 805346270725.28442383\n",
      "Iteration 91, loss = 796859960071.15649414\n",
      "Iteration 92, loss = 788645804501.15856934\n",
      "Iteration 93, loss = 780527392247.98913574\n",
      "Iteration 94, loss = 772374792782.56433105\n",
      "Iteration 95, loss = 765200923817.28186035\n",
      "Iteration 96, loss = 756991500346.31311035\n",
      "Iteration 97, loss = 749886257475.78332520\n",
      "Iteration 98, loss = 742819568886.42956543\n",
      "Iteration 99, loss = 735021092447.31701660\n",
      "Iteration 100, loss = 728533001551.63598633\n",
      "Iteration 101, loss = 721589576728.54992676\n",
      "Iteration 102, loss = 714814091385.44042969\n",
      "Iteration 103, loss = 708528282026.61547852\n",
      "Iteration 104, loss = 702034696030.40502930\n",
      "Iteration 105, loss = 695451919400.32482910\n",
      "Iteration 106, loss = 689448123628.35913086\n",
      "Iteration 107, loss = 683551435128.09106445\n",
      "Iteration 108, loss = 677811458456.33740234\n",
      "Iteration 109, loss = 671475010002.51074219\n",
      "Iteration 110, loss = 665727795585.44519043\n",
      "Iteration 111, loss = 660592202476.48217773\n",
      "Iteration 112, loss = 654775544895.61706543\n",
      "Iteration 113, loss = 649636709754.29577637\n",
      "Iteration 114, loss = 644209555348.36730957\n",
      "Iteration 115, loss = 639160902372.61535645\n",
      "Iteration 116, loss = 634165140691.18017578\n",
      "Iteration 117, loss = 628760095026.73303223\n",
      "Iteration 118, loss = 623935424823.87463379\n",
      "Iteration 119, loss = 619075905651.12841797\n",
      "Iteration 120, loss = 614343699488.30078125\n",
      "Iteration 121, loss = 609912578331.39086914\n",
      "Iteration 122, loss = 604963669800.86437988\n",
      "Iteration 123, loss = 600620584272.35937500\n",
      "Iteration 124, loss = 596259703911.40734863\n",
      "Iteration 125, loss = 592086686258.49633789\n",
      "Iteration 126, loss = 588269085942.87878418\n",
      "Iteration 127, loss = 584034092256.98681641\n",
      "Iteration 128, loss = 580211273816.47485352\n",
      "Iteration 129, loss = 576096166521.20117188\n",
      "Iteration 130, loss = 572683209718.32690430\n",
      "Iteration 131, loss = 568984201300.95556641\n",
      "Iteration 132, loss = 565463210839.22644043\n",
      "Iteration 133, loss = 561967920838.03173828\n",
      "Iteration 134, loss = 558312030958.60864258\n",
      "Iteration 135, loss = 554943986354.93359375\n",
      "Iteration 136, loss = 552140391050.69409180\n",
      "Iteration 137, loss = 548488200247.49566650\n",
      "Iteration 138, loss = 545290638791.80847168\n",
      "Iteration 139, loss = 542600359927.53021240\n",
      "Iteration 140, loss = 539884957180.78002930\n",
      "Iteration 141, loss = 536597800851.50604248\n",
      "Iteration 142, loss = 533623526922.83154297\n",
      "Iteration 143, loss = 531416034703.59442139\n",
      "Iteration 144, loss = 528330699826.59802246\n",
      "Iteration 145, loss = 525647910267.43005371\n",
      "Iteration 146, loss = 523144386106.46594238\n",
      "Iteration 147, loss = 520654817856.01403809\n",
      "Iteration 148, loss = 518446331604.73376465\n",
      "Iteration 149, loss = 516041293811.69390869\n",
      "Iteration 150, loss = 513535458826.66882324\n",
      "Iteration 151, loss = 511782944656.92358398\n",
      "Iteration 152, loss = 509174551140.72821045\n",
      "Iteration 153, loss = 506968130245.86877441\n",
      "Iteration 154, loss = 505240210523.54754639\n",
      "Iteration 155, loss = 502614437097.69781494\n",
      "Iteration 156, loss = 500877166168.55798340\n",
      "Iteration 157, loss = 498893255523.93908691\n",
      "Iteration 158, loss = 497327304666.07220459\n",
      "Iteration 159, loss = 494984580253.72235107\n",
      "Iteration 160, loss = 493107201533.82696533\n",
      "Iteration 161, loss = 491664002434.96942139\n",
      "Iteration 162, loss = 490060253233.86663818\n",
      "Iteration 163, loss = 488336245119.41656494\n",
      "Iteration 164, loss = 486347484118.82513428\n",
      "Iteration 165, loss = 485085273304.02313232\n",
      "Iteration 166, loss = 483196987405.26336670\n",
      "Iteration 167, loss = 482003996351.22369385\n",
      "Iteration 168, loss = 480508031993.98480225\n",
      "Iteration 169, loss = 478824445531.24755859\n",
      "Iteration 170, loss = 477608326585.96160889\n",
      "Iteration 171, loss = 475802964713.32379150\n",
      "Iteration 172, loss = 474832801030.51507568\n",
      "Iteration 173, loss = 473454098061.73455811\n",
      "Iteration 174, loss = 472111340526.67608643\n",
      "Iteration 175, loss = 470957321105.30126953\n",
      "Iteration 176, loss = 469593459835.00482178\n",
      "Iteration 177, loss = 468609451073.23291016\n",
      "Iteration 178, loss = 467469689709.59918213\n",
      "Iteration 179, loss = 466644600010.13079834\n",
      "Iteration 180, loss = 465322705296.02935791\n",
      "Iteration 181, loss = 464540950959.07495117\n",
      "Iteration 182, loss = 462971698322.71343994\n",
      "Iteration 183, loss = 462346301859.85430908\n",
      "Iteration 184, loss = 461309058352.46374512\n",
      "Iteration 185, loss = 460161128365.83966064\n",
      "Iteration 186, loss = 458986379993.52673340\n",
      "Iteration 187, loss = 459156160234.45220947\n",
      "Iteration 188, loss = 457597164312.68249512\n",
      "Iteration 189, loss = 456517013551.40478516\n",
      "Iteration 190, loss = 455742442945.51831055\n",
      "Iteration 191, loss = 454834337642.12145996\n",
      "Iteration 192, loss = 454019930099.87194824\n",
      "Iteration 193, loss = 453294523624.97326660\n",
      "Iteration 194, loss = 452574945283.50909424\n",
      "Iteration 195, loss = 451379071586.73327637\n",
      "Iteration 196, loss = 450763200568.65283203\n",
      "Iteration 197, loss = 450730552670.78601074\n",
      "Iteration 198, loss = 449474278748.78607178\n",
      "Iteration 199, loss = 448971909786.74304199\n",
      "Iteration 200, loss = 447965135430.25903320\n",
      "Iteration 201, loss = 447281760809.24615479\n",
      "Iteration 202, loss = 446462449521.16748047\n",
      "Iteration 203, loss = 445725836628.46942139\n",
      "Iteration 204, loss = 445328543427.48522949\n",
      "Iteration 205, loss = 445154054639.92767334\n",
      "Iteration 206, loss = 444000044401.35382080\n",
      "Iteration 207, loss = 443247460557.29962158\n",
      "Iteration 208, loss = 442479468627.45233154\n",
      "Iteration 209, loss = 442028706142.77093506\n",
      "Iteration 210, loss = 441661697345.84216309\n",
      "Iteration 211, loss = 440876899804.35131836\n",
      "Iteration 212, loss = 440147976293.38000488\n",
      "Iteration 213, loss = 440008282179.64862061\n",
      "Iteration 214, loss = 439141111137.46728516\n",
      "Iteration 215, loss = 438512610837.51092529\n",
      "Iteration 216, loss = 438524098855.35516357\n",
      "Iteration 217, loss = 438177427549.40460205\n",
      "Iteration 218, loss = 437097467566.69879150\n",
      "Iteration 219, loss = 436843041147.49530029\n",
      "Iteration 220, loss = 435896382312.23016357\n",
      "Iteration 221, loss = 435591251318.69970703\n",
      "Iteration 222, loss = 435404525176.77716064\n",
      "Iteration 223, loss = 435610091290.23803711\n",
      "Iteration 224, loss = 433936764389.96429443\n",
      "Iteration 225, loss = 433542109216.95520020\n",
      "Iteration 226, loss = 433270334691.73504639\n",
      "Iteration 227, loss = 432391091284.53729248\n",
      "Iteration 228, loss = 432345711146.05700684\n",
      "Iteration 229, loss = 431638151959.15625000\n",
      "Iteration 230, loss = 431544094831.98547363\n",
      "Iteration 231, loss = 430621025911.47241211\n",
      "Iteration 232, loss = 430756495547.25628662\n",
      "Iteration 233, loss = 430109777760.05633545\n",
      "Iteration 234, loss = 429468876822.03857422\n",
      "Iteration 235, loss = 429386817108.47991943\n",
      "Iteration 236, loss = 428693650101.53417969\n",
      "Iteration 237, loss = 428528308737.89129639\n",
      "Iteration 238, loss = 427695077697.19030762\n",
      "Iteration 239, loss = 427499426540.25933838\n",
      "Iteration 240, loss = 426918793074.96209717\n",
      "Iteration 241, loss = 426749485476.92749023\n",
      "Iteration 242, loss = 426708662326.71258545\n",
      "Iteration 243, loss = 425958648668.17016602\n",
      "Iteration 244, loss = 425481555044.83868408\n",
      "Iteration 245, loss = 426502368747.79357910\n",
      "Iteration 246, loss = 424680531476.44750977\n",
      "Iteration 247, loss = 424365713688.78198242\n",
      "Iteration 248, loss = 423940311706.29443359\n",
      "Iteration 249, loss = 424003399414.78271484\n",
      "Iteration 250, loss = 423740740314.37207031\n",
      "Iteration 251, loss = 422932450245.75329590\n",
      "Iteration 252, loss = 422811947255.67584229\n",
      "Iteration 253, loss = 422404942499.92492676\n",
      "Iteration 254, loss = 422276955115.87341309\n",
      "Iteration 255, loss = 422038373418.88146973\n",
      "Iteration 256, loss = 421516285148.94537354\n",
      "Iteration 257, loss = 421262466633.59136963\n",
      "Iteration 258, loss = 420650467500.03521729\n",
      "Iteration 259, loss = 420578488446.88378906\n",
      "Iteration 260, loss = 420400929534.78442383\n",
      "Iteration 261, loss = 420155407655.58587646\n",
      "Iteration 262, loss = 419466605799.83428955\n",
      "Iteration 263, loss = 419573092761.12207031\n",
      "Iteration 264, loss = 419159486532.38317871\n",
      "Iteration 265, loss = 419050370894.14654541\n",
      "Iteration 266, loss = 419021265489.85382080\n",
      "Iteration 267, loss = 418603847086.33703613\n",
      "Iteration 268, loss = 418079063084.98046875\n",
      "Iteration 269, loss = 418006135769.62957764\n",
      "Iteration 270, loss = 417115278716.19488525\n",
      "Iteration 271, loss = 417361813196.40582275\n",
      "Iteration 272, loss = 416682616016.34417725\n",
      "Iteration 273, loss = 416339493878.54345703\n",
      "Iteration 274, loss = 416057480674.19201660\n",
      "Iteration 275, loss = 416243879515.96539307\n",
      "Iteration 276, loss = 415825621009.95642090\n",
      "Iteration 277, loss = 415581169868.85205078\n",
      "Iteration 278, loss = 415182499777.39239502\n",
      "Iteration 279, loss = 414769705544.29016113\n",
      "Iteration 280, loss = 414236459998.51434326\n",
      "Iteration 281, loss = 414048943991.61315918\n",
      "Iteration 282, loss = 413876586980.16625977\n",
      "Iteration 283, loss = 413614951540.46569824\n",
      "Iteration 284, loss = 413236897189.16253662\n",
      "Iteration 285, loss = 412901608206.31457520\n",
      "Iteration 286, loss = 413215347348.47137451\n",
      "Iteration 287, loss = 412848238199.26281738\n",
      "Iteration 288, loss = 412546164378.82104492\n",
      "Iteration 289, loss = 412236577938.96710205\n",
      "Iteration 290, loss = 411963979934.36260986\n",
      "Iteration 291, loss = 411827907127.57781982\n",
      "Iteration 292, loss = 411370058923.95605469\n",
      "Iteration 293, loss = 411209171235.52496338\n",
      "Iteration 294, loss = 410672263017.48883057\n",
      "Iteration 295, loss = 410775395381.58746338\n",
      "Iteration 296, loss = 411071952821.63885498\n",
      "Iteration 297, loss = 410080398993.58770752\n",
      "Iteration 298, loss = 410035161328.66119385\n",
      "Iteration 299, loss = 411071609957.79388428\n",
      "Iteration 300, loss = 409459873429.57806396\n",
      "Iteration 301, loss = 409328155674.48999023\n",
      "Iteration 302, loss = 409389394197.18383789\n",
      "Iteration 303, loss = 408986627610.01098633\n",
      "Iteration 304, loss = 408873025523.55151367\n",
      "Iteration 305, loss = 408427157192.09448242\n",
      "Iteration 306, loss = 408104669532.38220215\n",
      "Iteration 307, loss = 408462950547.31561279\n",
      "Iteration 308, loss = 407937063776.49066162\n",
      "Iteration 309, loss = 407516037060.64392090\n",
      "Iteration 310, loss = 407264246100.48236084\n",
      "Iteration 311, loss = 407288981422.66406250\n",
      "Iteration 312, loss = 407070765173.43017578\n",
      "Iteration 313, loss = 407086156366.60095215\n",
      "Iteration 314, loss = 406817933554.05438232\n",
      "Iteration 315, loss = 407182626124.38067627\n",
      "Iteration 316, loss = 405882893185.60516357\n",
      "Iteration 317, loss = 405739700611.52355957\n",
      "Iteration 318, loss = 405725975708.90789795\n",
      "Iteration 319, loss = 405406191135.39794922\n",
      "Iteration 320, loss = 405242507551.36834717\n",
      "Iteration 321, loss = 405182720947.03356934\n",
      "Iteration 322, loss = 404787092397.69757080\n",
      "Iteration 323, loss = 404801931402.01940918\n",
      "Iteration 324, loss = 405188828172.75732422\n",
      "Iteration 325, loss = 404643285320.64819336\n",
      "Iteration 326, loss = 403940296684.33654785\n",
      "Iteration 327, loss = 403961163180.80609131\n",
      "Iteration 328, loss = 403547380983.71539307\n",
      "Iteration 329, loss = 403388397794.27239990\n",
      "Iteration 330, loss = 403290387804.31439209\n",
      "Iteration 331, loss = 403197097074.06646729\n",
      "Iteration 332, loss = 402882348685.20043945\n",
      "Iteration 333, loss = 403477354962.57080078\n",
      "Iteration 334, loss = 402402218275.76361084\n",
      "Iteration 335, loss = 402757775220.10803223\n",
      "Iteration 336, loss = 402835111476.07556152\n",
      "Iteration 337, loss = 401798217690.18872070\n",
      "Iteration 338, loss = 401989720079.73596191\n",
      "Iteration 339, loss = 402252618190.54003906\n",
      "Iteration 340, loss = 401364374988.42645264\n",
      "Iteration 341, loss = 401260561155.65673828\n",
      "Iteration 342, loss = 401073606861.64440918\n",
      "Iteration 343, loss = 401085833981.07702637\n",
      "Iteration 344, loss = 401127399752.26617432\n",
      "Iteration 345, loss = 401817047037.84301758\n",
      "Iteration 346, loss = 400596964689.61157227\n",
      "Iteration 347, loss = 400288193285.64965820\n",
      "Iteration 348, loss = 399646734475.22534180\n",
      "Iteration 349, loss = 399561035363.94183350\n",
      "Iteration 350, loss = 400331079521.21832275\n",
      "Iteration 351, loss = 399479224045.01141357\n",
      "Iteration 352, loss = 399205696563.17095947\n",
      "Iteration 353, loss = 398839627433.19091797\n",
      "Iteration 354, loss = 398933437215.21234131\n",
      "Iteration 355, loss = 398819470382.04583740\n",
      "Iteration 356, loss = 398869564479.68005371\n",
      "Iteration 357, loss = 398256520844.39050293\n",
      "Iteration 358, loss = 398142133634.76385498\n",
      "Iteration 359, loss = 398323465075.78753662\n",
      "Iteration 360, loss = 397383942222.69421387\n",
      "Iteration 361, loss = 397378145444.41149902\n",
      "Iteration 362, loss = 397251062548.37518311\n",
      "Iteration 363, loss = 397125735287.49554443\n",
      "Iteration 364, loss = 397044673936.66955566\n",
      "Iteration 365, loss = 397024712666.77478027\n",
      "Iteration 366, loss = 396323388157.07958984\n",
      "Iteration 367, loss = 396911585689.48571777\n",
      "Iteration 368, loss = 396337083568.62908936\n",
      "Iteration 369, loss = 396309568896.35308838\n",
      "Iteration 370, loss = 395698834978.63909912\n",
      "Iteration 371, loss = 396212696388.93609619\n",
      "Iteration 372, loss = 395443964370.56842041\n",
      "Iteration 373, loss = 396036684531.57806396\n",
      "Iteration 374, loss = 394784895405.27484131\n",
      "Iteration 375, loss = 395743275738.75671387\n",
      "Iteration 376, loss = 395189628241.45965576\n",
      "Iteration 377, loss = 394458276755.20770264\n",
      "Iteration 378, loss = 394951400415.13000488\n",
      "Iteration 379, loss = 394936430889.61993408\n",
      "Iteration 380, loss = 394469914364.10058594\n",
      "Iteration 381, loss = 394596736809.31018066\n",
      "Iteration 382, loss = 393879313856.58905029\n",
      "Iteration 383, loss = 393640740336.31817627\n",
      "Iteration 384, loss = 393950862273.93945312\n",
      "Iteration 385, loss = 393490868591.36950684\n",
      "Iteration 386, loss = 393291302511.90570068\n",
      "Iteration 387, loss = 393587220371.18957520\n",
      "Iteration 388, loss = 394323163194.43127441\n",
      "Iteration 389, loss = 392728585940.79455566\n",
      "Iteration 390, loss = 392782651963.48095703\n",
      "Iteration 391, loss = 393072791253.73663330\n",
      "Iteration 392, loss = 392342840111.81323242\n",
      "Iteration 393, loss = 392444693846.40692139\n",
      "Iteration 394, loss = 392119873730.28002930\n",
      "Iteration 395, loss = 392176206429.01525879\n",
      "Iteration 396, loss = 392467996477.35736084\n",
      "Iteration 397, loss = 391916299104.70391846\n",
      "Iteration 398, loss = 391442905845.43579102\n",
      "Iteration 399, loss = 391639109252.88452148\n",
      "Iteration 400, loss = 390924325511.80151367\n",
      "Iteration 401, loss = 391425403588.83154297\n",
      "Iteration 402, loss = 390686393897.26599121\n",
      "Iteration 403, loss = 391192496951.18505859\n",
      "Iteration 404, loss = 390627918306.90911865\n",
      "Iteration 405, loss = 390386544999.60327148\n",
      "Iteration 406, loss = 390373280189.84429932\n",
      "Iteration 407, loss = 390784641000.89172363\n",
      "Iteration 408, loss = 390192813654.01354980\n",
      "Iteration 409, loss = 390026067147.82531738\n",
      "Iteration 410, loss = 390033992107.71801758\n",
      "Iteration 411, loss = 389691929141.62365723\n",
      "Iteration 412, loss = 389922021458.92883301\n",
      "Iteration 413, loss = 389556585148.17541504\n",
      "Iteration 414, loss = 389603338227.36932373\n",
      "Iteration 415, loss = 389031594586.06060791\n",
      "Iteration 416, loss = 389086155240.93981934\n",
      "Iteration 417, loss = 388868084838.74932861\n",
      "Iteration 418, loss = 388729562552.17987061\n",
      "Iteration 419, loss = 388219759813.75708008\n",
      "Iteration 420, loss = 388260833744.72131348\n",
      "Iteration 421, loss = 388180113653.56207275\n",
      "Iteration 422, loss = 387923808440.48803711\n",
      "Iteration 423, loss = 387766204764.07946777\n",
      "Iteration 424, loss = 387677651922.43249512\n",
      "Iteration 425, loss = 387797748600.10058594\n",
      "Iteration 426, loss = 387362710124.94891357\n",
      "Iteration 427, loss = 387784140666.53625488\n",
      "Iteration 428, loss = 387163209650.48449707\n",
      "Iteration 429, loss = 386675830509.16528320\n",
      "Iteration 430, loss = 386584833480.40527344\n",
      "Iteration 431, loss = 386618420029.27551270\n",
      "Iteration 432, loss = 386855317042.54083252\n",
      "Iteration 433, loss = 386333315568.48382568\n",
      "Iteration 434, loss = 386224834825.78332520\n",
      "Iteration 435, loss = 386476051917.55383301\n",
      "Iteration 436, loss = 386014495342.86596680\n",
      "Iteration 437, loss = 386087218094.14953613\n",
      "Iteration 438, loss = 385627494508.88909912\n",
      "Iteration 439, loss = 385672077551.69824219\n",
      "Iteration 440, loss = 385550627917.15344238\n",
      "Iteration 441, loss = 384961595869.11309814\n",
      "Iteration 442, loss = 384892079851.58953857\n",
      "Iteration 443, loss = 385241721651.31225586\n",
      "Iteration 444, loss = 384348127546.54705811\n",
      "Iteration 445, loss = 384954061260.77575684\n",
      "Iteration 446, loss = 384426619747.82281494\n",
      "Iteration 447, loss = 384587421986.90155029\n",
      "Iteration 448, loss = 384449574641.29821777\n",
      "Iteration 449, loss = 385055877503.33905029\n",
      "Iteration 450, loss = 384928216430.92846680\n",
      "Iteration 451, loss = 383729901075.07421875\n",
      "Iteration 452, loss = 383636859023.47399902\n",
      "Iteration 453, loss = 383563702662.04846191\n",
      "Iteration 454, loss = 384096278564.22497559\n",
      "Iteration 455, loss = 385065568908.30505371\n",
      "Iteration 456, loss = 382921404555.93884277\n",
      "Iteration 457, loss = 383510444609.96691895\n",
      "Iteration 458, loss = 383210813042.92517090\n",
      "Iteration 459, loss = 382627587382.92700195\n",
      "Iteration 460, loss = 383602737620.86761475\n",
      "Iteration 461, loss = 383163162305.22955322\n",
      "Iteration 462, loss = 382305438271.73718262\n",
      "Iteration 463, loss = 382560741535.72393799\n",
      "Iteration 464, loss = 382173366451.70227051\n",
      "Iteration 465, loss = 382258036942.90295410\n",
      "Iteration 466, loss = 381796736400.07055664\n",
      "Iteration 467, loss = 381547006281.17657471\n",
      "Iteration 468, loss = 381561143594.08569336\n",
      "Iteration 469, loss = 381257562696.10980225\n",
      "Iteration 470, loss = 381673548854.28936768\n",
      "Iteration 471, loss = 381195789395.13629150\n",
      "Iteration 472, loss = 381935406052.41992188\n",
      "Iteration 473, loss = 381445571624.05969238\n",
      "Iteration 474, loss = 380625072737.60760498\n",
      "Iteration 475, loss = 380569104122.71759033\n",
      "Iteration 476, loss = 380604815160.67419434\n",
      "Iteration 477, loss = 381009636824.11541748\n",
      "Iteration 478, loss = 380399804921.84771729\n",
      "Iteration 479, loss = 380693585275.71343994\n",
      "Iteration 480, loss = 380095073838.90362549\n",
      "Iteration 481, loss = 380100133368.62579346\n",
      "Iteration 482, loss = 380313279864.12097168\n",
      "Iteration 483, loss = 379866789897.68798828\n",
      "Iteration 484, loss = 379786071124.88665771\n",
      "Iteration 485, loss = 379422001983.19158936\n",
      "Iteration 486, loss = 379488821633.01568604\n",
      "Iteration 487, loss = 379634418139.92254639\n",
      "Iteration 488, loss = 379235937429.32342529\n",
      "Iteration 489, loss = 379325192760.23144531\n",
      "Iteration 490, loss = 379418300563.00708008\n",
      "Iteration 491, loss = 378704201983.15216064\n",
      "Iteration 492, loss = 378817020156.82843018\n",
      "Iteration 493, loss = 378831405720.80487061\n",
      "Iteration 494, loss = 378213259940.01391602\n",
      "Iteration 495, loss = 378678701427.25311279\n",
      "Iteration 496, loss = 378924425987.34039307\n",
      "Iteration 497, loss = 378068325003.51141357\n",
      "Iteration 498, loss = 377995104335.54150391\n",
      "Iteration 499, loss = 378176691857.67681885\n",
      "Iteration 500, loss = 377309409212.26196289\n",
      "Iteration 501, loss = 377712512281.63745117\n",
      "Iteration 502, loss = 377057850447.94378662\n",
      "Iteration 503, loss = 377478885616.43511963\n",
      "Iteration 504, loss = 377291494790.60528564\n",
      "Iteration 505, loss = 377097882157.04180908\n",
      "Iteration 506, loss = 377248892959.04870605\n",
      "Iteration 507, loss = 376793574467.78356934\n",
      "Iteration 508, loss = 376505968496.80218506\n",
      "Iteration 509, loss = 376633129536.95104980\n",
      "Iteration 510, loss = 376275300680.80255127\n",
      "Iteration 511, loss = 377175244786.64880371\n",
      "Iteration 512, loss = 376037612281.61029053\n",
      "Iteration 513, loss = 376355217408.73040771\n",
      "Iteration 514, loss = 375896032247.57135010\n",
      "Iteration 515, loss = 376438232776.93750000\n",
      "Iteration 516, loss = 376607350971.03570557\n",
      "Iteration 517, loss = 375588766438.55340576\n",
      "Iteration 518, loss = 375803297321.52294922\n",
      "Iteration 519, loss = 375433620648.81756592\n",
      "Iteration 520, loss = 375406274999.07250977\n",
      "Iteration 521, loss = 375686858320.33178711\n",
      "Iteration 522, loss = 375263658325.06408691\n",
      "Iteration 523, loss = 375213479678.48901367\n",
      "Iteration 524, loss = 375422509118.83538818\n",
      "Iteration 525, loss = 374940557089.96563721\n",
      "Iteration 526, loss = 375374045091.43896484\n",
      "Iteration 527, loss = 374872498881.26068115\n",
      "Iteration 528, loss = 374719115469.14031982\n",
      "Iteration 529, loss = 374606736098.01507568\n",
      "Iteration 530, loss = 374932729864.09661865\n",
      "Iteration 531, loss = 373875519784.38592529\n",
      "Iteration 532, loss = 373871492741.71795654\n",
      "Iteration 533, loss = 374061878159.17938232\n",
      "Iteration 534, loss = 373639110989.96569824\n",
      "Iteration 535, loss = 373506851750.90173340\n",
      "Iteration 536, loss = 373820782757.94580078\n",
      "Iteration 537, loss = 373289153889.53265381\n",
      "Iteration 538, loss = 373968184941.27606201\n",
      "Iteration 539, loss = 372899205368.83831787\n",
      "Iteration 540, loss = 372974589845.34533691\n",
      "Iteration 541, loss = 372901817513.92028809\n",
      "Iteration 542, loss = 373174609968.84808350\n",
      "Iteration 543, loss = 372783711055.55035400\n",
      "Iteration 544, loss = 372219875772.62921143\n",
      "Iteration 545, loss = 373211438640.78594971\n",
      "Iteration 546, loss = 372025044538.78723145\n",
      "Iteration 547, loss = 373701960738.73693848\n",
      "Iteration 548, loss = 371823313957.89245605\n",
      "Iteration 549, loss = 372700569051.90692139\n",
      "Iteration 550, loss = 373157723889.30126953\n",
      "Iteration 551, loss = 371694131737.01531982\n",
      "Iteration 552, loss = 372308392804.70629883\n",
      "Iteration 553, loss = 371608085083.64880371\n",
      "Iteration 554, loss = 372152539436.31610107\n",
      "Iteration 555, loss = 371242700952.92529297\n",
      "Iteration 556, loss = 371616859305.03625488\n",
      "Iteration 557, loss = 371764396084.41699219\n",
      "Iteration 558, loss = 371037081519.24072266\n",
      "Iteration 559, loss = 371067327116.37518311\n",
      "Iteration 560, loss = 370827579734.54620361\n",
      "Iteration 561, loss = 371018860616.56988525\n",
      "Iteration 562, loss = 370686576771.44799805\n",
      "Iteration 563, loss = 370587770470.30493164\n",
      "Iteration 564, loss = 370316639863.34991455\n",
      "Iteration 565, loss = 369957199336.41058350\n",
      "Iteration 566, loss = 369926256655.59515381\n",
      "Iteration 567, loss = 370149886347.93463135\n",
      "Iteration 568, loss = 369749193262.27819824\n",
      "Iteration 569, loss = 370114596609.41577148\n",
      "Iteration 570, loss = 369395784026.69482422\n",
      "Iteration 571, loss = 369906240939.48229980\n",
      "Iteration 572, loss = 369607934025.41717529\n",
      "Iteration 573, loss = 369368493043.86987305\n",
      "Iteration 574, loss = 369365924428.37231445\n",
      "Iteration 575, loss = 369333898108.21533203\n",
      "Iteration 576, loss = 369766472556.58758545\n",
      "Iteration 577, loss = 368536651425.13958740\n",
      "Iteration 578, loss = 368815931936.70928955\n",
      "Iteration 579, loss = 368536468810.13909912\n",
      "Iteration 580, loss = 368800382652.48175049\n",
      "Iteration 581, loss = 368304627112.73968506\n",
      "Iteration 582, loss = 369262107721.84124756\n",
      "Iteration 583, loss = 368350024455.02673340\n",
      "Iteration 584, loss = 367992578258.47570801\n",
      "Iteration 585, loss = 368823870101.32775879\n",
      "Iteration 586, loss = 367821464225.14404297\n",
      "Iteration 587, loss = 367927121104.22033691\n",
      "Iteration 588, loss = 367908551606.75531006\n",
      "Iteration 589, loss = 367953372421.94067383\n",
      "Iteration 590, loss = 367636993480.89794922\n",
      "Iteration 591, loss = 367342424377.30603027\n",
      "Iteration 592, loss = 367557976975.03967285\n",
      "Iteration 593, loss = 367250629588.15368652\n",
      "Iteration 594, loss = 367366654326.46276855\n",
      "Iteration 595, loss = 367098996481.49249268\n",
      "Iteration 596, loss = 367433275642.33477783\n",
      "Iteration 597, loss = 366773575549.28442383\n",
      "Iteration 598, loss = 367086170238.97180176\n",
      "Iteration 599, loss = 367271525327.41088867\n",
      "Iteration 600, loss = 367398415605.38397217\n",
      "Iteration 601, loss = 366607169592.89770508\n",
      "Iteration 602, loss = 366475797405.29559326\n",
      "Iteration 603, loss = 366628836724.79541016\n",
      "Iteration 604, loss = 366431644550.36456299\n",
      "Iteration 605, loss = 366362017284.40875244\n",
      "Iteration 606, loss = 366827390683.10791016\n",
      "Iteration 607, loss = 365579963833.89398193\n",
      "Iteration 608, loss = 366617958660.17041016\n",
      "Iteration 609, loss = 366286754666.69421387\n",
      "Iteration 610, loss = 365802082281.91595459\n",
      "Iteration 611, loss = 365674375395.93481445\n",
      "Iteration 612, loss = 365699779486.40618896\n",
      "Iteration 613, loss = 366715049432.56207275\n",
      "Iteration 614, loss = 365552931455.67907715\n",
      "Iteration 615, loss = 365223854321.12719727\n",
      "Iteration 616, loss = 365444193354.38580322\n",
      "Iteration 617, loss = 365261853911.09960938\n",
      "Iteration 618, loss = 365477776023.83856201\n",
      "Iteration 619, loss = 365360454899.50891113\n",
      "Iteration 620, loss = 365674590929.00482178\n",
      "Iteration 621, loss = 364711613294.49078369\n",
      "Iteration 622, loss = 364656530717.50177002\n",
      "Iteration 623, loss = 364897829109.54656982\n",
      "Iteration 624, loss = 365120811015.85687256\n",
      "Iteration 625, loss = 364334502484.35290527\n",
      "Iteration 626, loss = 364488352654.29937744\n",
      "Iteration 627, loss = 364806367066.82165527\n",
      "Iteration 628, loss = 364381615405.57354736\n",
      "Iteration 629, loss = 363943797924.88781738\n",
      "Iteration 630, loss = 363741503515.86718750\n",
      "Iteration 631, loss = 364286984526.47454834\n",
      "Iteration 632, loss = 363794129663.94738770\n",
      "Iteration 633, loss = 364319411784.13366699\n",
      "Iteration 634, loss = 363443039548.18505859\n",
      "Iteration 635, loss = 363667153371.77850342\n",
      "Iteration 636, loss = 363571318983.87078857\n",
      "Iteration 637, loss = 363325843960.03753662\n",
      "Iteration 638, loss = 364856365467.02954102\n",
      "Iteration 639, loss = 363741901732.82983398\n",
      "Iteration 640, loss = 363185962632.42694092\n",
      "Iteration 641, loss = 363315049814.79113770\n",
      "Iteration 642, loss = 363255414120.82336426\n",
      "Iteration 643, loss = 362819153018.99237061\n",
      "Iteration 644, loss = 362743847512.29730225\n",
      "Iteration 645, loss = 363081328968.50567627\n",
      "Iteration 646, loss = 362903495247.31506348\n",
      "Iteration 647, loss = 363573739926.78442383\n",
      "Iteration 648, loss = 362221091490.50616455\n",
      "Iteration 649, loss = 362691847540.01245117\n",
      "Iteration 650, loss = 362274142931.96643066\n",
      "Iteration 651, loss = 362325870535.52453613\n",
      "Iteration 652, loss = 362292131210.26538086\n",
      "Iteration 653, loss = 362510975515.66632080\n",
      "Iteration 654, loss = 362733298594.63073730\n",
      "Iteration 655, loss = 361581618116.09991455\n",
      "Iteration 656, loss = 362124693483.65783691\n",
      "Iteration 657, loss = 362127430170.23364258\n",
      "Iteration 658, loss = 362189556078.98132324\n",
      "Iteration 659, loss = 361845260983.71917725\n",
      "Iteration 660, loss = 361641138432.30303955\n",
      "Iteration 661, loss = 361102527884.40545654\n",
      "Iteration 662, loss = 361166102252.71429443\n",
      "Iteration 663, loss = 361696057526.83959961\n",
      "Iteration 664, loss = 361402694683.73693848\n",
      "Iteration 665, loss = 361395288109.87103271\n",
      "Iteration 666, loss = 360837377067.18927002\n",
      "Iteration 667, loss = 361091130794.25994873\n",
      "Iteration 668, loss = 361394652247.48785400\n",
      "Iteration 669, loss = 360382262789.62231445\n",
      "Iteration 670, loss = 360859440427.38952637\n",
      "Iteration 671, loss = 361232974133.54235840\n",
      "Iteration 672, loss = 360478602297.74914551\n",
      "Iteration 673, loss = 360458594011.99163818\n",
      "Iteration 674, loss = 360349484459.00903320\n",
      "Iteration 675, loss = 361282500489.96893311\n",
      "Iteration 676, loss = 360648565889.72137451\n",
      "Iteration 677, loss = 360538697982.26513672\n",
      "Iteration 678, loss = 360387897433.03094482\n",
      "Iteration 679, loss = 360746132674.31243896\n",
      "Iteration 680, loss = 360095987360.39544678\n",
      "Iteration 681, loss = 360094615281.18804932\n",
      "Iteration 682, loss = 359931411709.07476807\n",
      "Iteration 683, loss = 360048575849.02630615\n",
      "Iteration 684, loss = 359838496066.92657471\n",
      "Iteration 685, loss = 362121663503.81732178\n",
      "Iteration 686, loss = 359401924412.16296387\n",
      "Iteration 687, loss = 359729254365.84393311\n",
      "Iteration 688, loss = 360104100484.25994873\n",
      "Iteration 689, loss = 360273687354.59606934\n",
      "Iteration 690, loss = 359939390013.61669922\n",
      "Iteration 691, loss = 359516103284.29998779\n",
      "Iteration 692, loss = 358874318251.06433105\n",
      "Iteration 693, loss = 359527284893.64031982\n",
      "Iteration 694, loss = 359004726067.09844971\n",
      "Iteration 695, loss = 358875477946.75286865\n",
      "Iteration 696, loss = 359729654026.98193359\n",
      "Iteration 697, loss = 359482467976.22698975\n",
      "Iteration 698, loss = 358837202724.58337402\n",
      "Iteration 699, loss = 358547475394.80541992\n",
      "Iteration 700, loss = 358329920194.84527588\n",
      "Iteration 701, loss = 358701151984.83178711\n",
      "Iteration 702, loss = 358730930007.55279541\n",
      "Iteration 703, loss = 358433600272.51232910\n",
      "Iteration 704, loss = 358607274167.64270020\n",
      "Iteration 705, loss = 358565311867.23681641\n",
      "Iteration 706, loss = 358118399334.52319336\n",
      "Iteration 707, loss = 357963852954.72283936\n",
      "Iteration 708, loss = 358065783977.90930176\n",
      "Iteration 709, loss = 357763873966.98394775\n",
      "Iteration 710, loss = 357738993279.65454102\n",
      "Iteration 711, loss = 357551354552.92462158\n",
      "Iteration 712, loss = 358145459685.02966309\n",
      "Iteration 713, loss = 357822477186.00640869\n",
      "Iteration 714, loss = 357747278049.25067139\n",
      "Iteration 715, loss = 357738844695.29296875\n",
      "Iteration 716, loss = 357457262058.86920166\n",
      "Iteration 717, loss = 357552602664.42181396\n",
      "Iteration 718, loss = 357692934327.55303955\n",
      "Iteration 719, loss = 357886942077.91815186\n",
      "Iteration 720, loss = 357272174636.79595947\n",
      "Iteration 721, loss = 356851156991.21179199\n",
      "Iteration 722, loss = 356991512303.53833008\n",
      "Iteration 723, loss = 356755012287.59942627\n",
      "Iteration 724, loss = 356817273918.51641846\n",
      "Iteration 725, loss = 356719157052.41595459\n",
      "Iteration 726, loss = 356778547497.61102295\n",
      "Iteration 727, loss = 356561448889.52593994\n",
      "Iteration 728, loss = 356034977541.93853760\n",
      "Iteration 729, loss = 356256804662.68566895\n",
      "Iteration 730, loss = 356276496937.95275879\n",
      "Iteration 731, loss = 356369657671.46966553\n",
      "Iteration 732, loss = 356539896959.81817627\n",
      "Iteration 733, loss = 356300036962.65313721\n",
      "Iteration 734, loss = 355808671089.80200195\n",
      "Iteration 735, loss = 356352540548.46447754\n",
      "Iteration 736, loss = 355807956461.41320801\n",
      "Iteration 737, loss = 356639522482.17517090\n",
      "Iteration 738, loss = 355597021073.33178711\n",
      "Iteration 739, loss = 356168831340.50384521\n",
      "Iteration 740, loss = 355956161570.06292725\n",
      "Iteration 741, loss = 355170802215.59735107\n",
      "Iteration 742, loss = 355054351809.11517334\n",
      "Iteration 743, loss = 355043950091.38287354\n",
      "Iteration 744, loss = 354988840218.21099854\n",
      "Iteration 745, loss = 355153114706.88433838\n",
      "Iteration 746, loss = 355100866735.99182129\n",
      "Iteration 747, loss = 354608640382.26055908\n",
      "Iteration 748, loss = 354802415901.87994385\n",
      "Iteration 749, loss = 354672824719.08245850\n",
      "Iteration 750, loss = 354938443135.20050049\n",
      "Iteration 751, loss = 355396721121.39569092\n",
      "Iteration 752, loss = 354735438078.78747559\n",
      "Iteration 753, loss = 354754875920.41125488\n",
      "Iteration 754, loss = 354613467810.94897461\n",
      "Iteration 755, loss = 354815088270.24279785\n",
      "Iteration 756, loss = 355223743323.27032471\n",
      "Iteration 757, loss = 354582763582.20635986\n",
      "Iteration 758, loss = 354083791686.34472656\n",
      "Iteration 759, loss = 354022124564.36206055\n",
      "Iteration 760, loss = 353846932221.70294189\n",
      "Iteration 761, loss = 354367695670.36462402\n",
      "Iteration 762, loss = 354710215902.73596191\n",
      "Iteration 763, loss = 353503423099.62634277\n",
      "Iteration 764, loss = 353285135706.44152832\n",
      "Iteration 765, loss = 353850170307.66882324\n",
      "Iteration 766, loss = 354133453128.97534180\n",
      "Iteration 767, loss = 353903483166.48559570\n",
      "Iteration 768, loss = 353539151252.52386475\n",
      "Iteration 769, loss = 353612932276.82684326\n",
      "Iteration 770, loss = 353327485942.67321777\n",
      "Iteration 771, loss = 353129495881.10571289\n",
      "Iteration 772, loss = 353184113435.78521729\n",
      "Iteration 773, loss = 353406958772.19451904\n",
      "Iteration 774, loss = 353212747624.19000244\n",
      "Iteration 775, loss = 353262360464.18432617\n",
      "Iteration 776, loss = 353735703954.41094971\n",
      "Iteration 777, loss = 353125318161.32427979\n",
      "Iteration 778, loss = 353141146125.91345215\n",
      "Iteration 779, loss = 353019772820.11810303\n",
      "Iteration 780, loss = 353268897471.36669922\n",
      "Iteration 781, loss = 352696084548.51983643\n",
      "Iteration 782, loss = 352535902302.64379883\n",
      "Iteration 783, loss = 352620817801.45556641\n",
      "Iteration 784, loss = 352843707555.32568359\n",
      "Iteration 785, loss = 352660250244.69274902\n",
      "Iteration 786, loss = 351843334794.19989014\n",
      "Iteration 787, loss = 352027054805.94696045\n",
      "Iteration 788, loss = 352191170828.61492920\n",
      "Iteration 789, loss = 352145282502.48419189\n",
      "Iteration 790, loss = 351939876090.32202148\n",
      "Iteration 791, loss = 351909256875.86932373\n",
      "Iteration 792, loss = 352094872061.62792969\n",
      "Iteration 793, loss = 351975354143.96166992\n",
      "Iteration 794, loss = 351677324414.59808350\n",
      "Iteration 795, loss = 351508444123.82531738\n",
      "Iteration 796, loss = 351299631280.90960693\n",
      "Iteration 797, loss = 352264510857.58819580\n",
      "Iteration 798, loss = 351719122741.77893066\n",
      "Iteration 799, loss = 351397717326.72381592\n",
      "Iteration 800, loss = 351465876160.79467773\n",
      "Iteration 801, loss = 351154228393.02178955\n",
      "Iteration 802, loss = 351474919436.62329102\n",
      "Iteration 803, loss = 351609023208.35968018\n",
      "Iteration 804, loss = 351115505494.96520996\n",
      "Iteration 805, loss = 350727799336.56945801\n",
      "Iteration 806, loss = 351230108665.27691650\n",
      "Iteration 807, loss = 351362304368.01281738\n",
      "Iteration 808, loss = 350466037300.58239746\n",
      "Iteration 809, loss = 350724804089.43206787\n",
      "Iteration 810, loss = 350536203739.95513916\n",
      "Iteration 811, loss = 350235927997.85241699\n",
      "Iteration 812, loss = 350337542717.04870605\n",
      "Iteration 813, loss = 350210656829.28820801\n",
      "Iteration 814, loss = 350381396573.15399170\n",
      "Iteration 815, loss = 350064620277.04907227\n",
      "Iteration 816, loss = 350144938015.21484375\n",
      "Iteration 817, loss = 349991225391.74865723\n",
      "Iteration 818, loss = 350179394683.13690186\n",
      "Iteration 819, loss = 350789372760.98358154\n",
      "Iteration 820, loss = 350328594403.15173340\n",
      "Iteration 821, loss = 349656131806.51629639\n",
      "Iteration 822, loss = 349666654657.50256348\n",
      "Iteration 823, loss = 349708097070.37341309\n",
      "Iteration 824, loss = 349790730435.96606445\n",
      "Iteration 825, loss = 349693706952.34570312\n",
      "Iteration 826, loss = 350593722402.42004395\n",
      "Iteration 827, loss = 349530924010.71453857\n",
      "Iteration 828, loss = 349562631892.29132080\n",
      "Iteration 829, loss = 349455381377.98132324\n",
      "Iteration 830, loss = 349122225391.97045898\n",
      "Iteration 831, loss = 349221803884.45721436\n",
      "Iteration 832, loss = 349017398464.66790771\n",
      "Iteration 833, loss = 349485289428.23852539\n",
      "Iteration 834, loss = 349205978788.12017822\n",
      "Iteration 835, loss = 348616903196.54589844\n",
      "Iteration 836, loss = 349486070956.60858154\n",
      "Iteration 837, loss = 349456028810.63800049\n",
      "Iteration 838, loss = 348909824793.17492676\n",
      "Iteration 839, loss = 348770165666.80627441\n",
      "Iteration 840, loss = 348808427222.70477295\n",
      "Iteration 841, loss = 348619379351.72003174\n",
      "Iteration 842, loss = 348342740296.96032715\n",
      "Iteration 843, loss = 348452751437.08331299\n",
      "Iteration 844, loss = 348281610585.02233887\n",
      "Iteration 845, loss = 348338584858.50610352\n",
      "Iteration 846, loss = 348153049150.44036865\n",
      "Iteration 847, loss = 348364436442.25683594\n",
      "Iteration 848, loss = 348048185108.81372070\n",
      "Iteration 849, loss = 348069213051.51757812\n",
      "Iteration 850, loss = 348053686822.70605469\n",
      "Iteration 851, loss = 347882278223.04956055\n",
      "Iteration 852, loss = 348316671357.22509766\n",
      "Iteration 853, loss = 347795559224.00604248\n",
      "Iteration 854, loss = 348042084966.72253418\n",
      "Iteration 855, loss = 347677113096.35351562\n",
      "Iteration 856, loss = 348181788000.32232666\n",
      "Iteration 857, loss = 347688663808.20916748\n",
      "Iteration 858, loss = 347572789741.25054932\n",
      "Iteration 859, loss = 347516172088.46264648\n",
      "Iteration 860, loss = 347165813407.71649170\n",
      "Iteration 861, loss = 347326600948.96838379\n",
      "Iteration 862, loss = 347345354560.53424072\n",
      "Iteration 863, loss = 346944897706.73712158\n",
      "Iteration 864, loss = 347112225255.31195068\n",
      "Iteration 865, loss = 347637663343.11608887\n",
      "Iteration 866, loss = 347601567631.66009521\n",
      "Iteration 867, loss = 346955609403.02032471\n",
      "Iteration 868, loss = 346820746909.36743164\n",
      "Iteration 869, loss = 346541088118.13061523\n",
      "Iteration 870, loss = 347034663397.80450439\n",
      "Iteration 871, loss = 346493855040.06158447\n",
      "Iteration 872, loss = 347244696876.75421143\n",
      "Iteration 873, loss = 346698517842.44219971\n",
      "Iteration 874, loss = 346561168884.29968262\n",
      "Iteration 875, loss = 346661566206.44287109\n",
      "Iteration 876, loss = 346718635832.43487549\n",
      "Iteration 877, loss = 346213881529.67144775\n",
      "Iteration 878, loss = 346928231514.59576416\n",
      "Iteration 879, loss = 346244320845.90777588\n",
      "Iteration 880, loss = 346294202484.58068848\n",
      "Iteration 881, loss = 346410586797.14636230\n",
      "Iteration 882, loss = 346066826131.14508057\n",
      "Iteration 883, loss = 345933234685.33630371\n",
      "Iteration 884, loss = 346141627501.22698975\n",
      "Iteration 885, loss = 345727043393.18969727\n",
      "Iteration 886, loss = 345646161493.94323730\n",
      "Iteration 887, loss = 346375086060.23864746\n",
      "Iteration 888, loss = 345834327766.80029297\n",
      "Iteration 889, loss = 346549192466.42523193\n",
      "Iteration 890, loss = 345440201378.33917236\n",
      "Iteration 891, loss = 345103663735.76055908\n",
      "Iteration 892, loss = 346597847995.32946777\n",
      "Iteration 893, loss = 345097405347.10864258\n",
      "Iteration 894, loss = 345401942298.18957520\n",
      "Iteration 895, loss = 345784182014.97125244\n",
      "Iteration 896, loss = 345536023989.37573242\n",
      "Iteration 897, loss = 345693648289.56127930\n",
      "Iteration 898, loss = 345414519868.78765869\n",
      "Iteration 899, loss = 345539983947.30200195\n",
      "Iteration 900, loss = 345416656101.01879883\n",
      "Iteration 901, loss = 345658255574.92871094\n",
      "Iteration 902, loss = 346010752547.17407227\n",
      "Iteration 903, loss = 345012994501.98773193\n",
      "Iteration 904, loss = 344960135564.23999023\n",
      "Iteration 905, loss = 344978806637.91809082\n",
      "Iteration 906, loss = 345098176486.67047119\n",
      "Iteration 907, loss = 344603032155.88085938\n",
      "Iteration 908, loss = 344782079796.05487061\n",
      "Iteration 909, loss = 344346146822.44042969\n",
      "Iteration 910, loss = 344206889757.75384521\n",
      "Iteration 911, loss = 344491626369.82989502\n",
      "Iteration 912, loss = 344717061442.05023193\n",
      "Iteration 913, loss = 344474151116.07666016\n",
      "Iteration 914, loss = 344171759135.43054199\n",
      "Iteration 915, loss = 345040366744.52935791\n",
      "Iteration 916, loss = 344992205279.04583740\n",
      "Iteration 917, loss = 344113289193.30999756\n",
      "Iteration 918, loss = 344020771915.66882324\n",
      "Iteration 919, loss = 344255472920.50231934\n",
      "Iteration 920, loss = 344190096432.12933350\n",
      "Iteration 921, loss = 344333378884.31408691\n",
      "Iteration 922, loss = 343604680637.35516357\n",
      "Iteration 923, loss = 344013322800.44970703\n",
      "Iteration 924, loss = 343785054665.41711426\n",
      "Iteration 925, loss = 343518136167.83367920\n",
      "Iteration 926, loss = 343856757217.09625244\n",
      "Iteration 927, loss = 343978515348.14453125\n",
      "Iteration 928, loss = 344379172594.32537842\n",
      "Iteration 929, loss = 343455614461.88891602\n",
      "Iteration 930, loss = 343435480130.70806885\n",
      "Iteration 931, loss = 343535700393.15979004\n",
      "Iteration 932, loss = 343335947974.39929199\n",
      "Iteration 933, loss = 344469374475.77770996\n",
      "Iteration 934, loss = 343845718721.06518555\n",
      "Iteration 935, loss = 343291747782.34069824\n",
      "Iteration 936, loss = 342913179942.32263184\n",
      "Iteration 937, loss = 342800463698.52783203\n",
      "Iteration 938, loss = 343286531102.17816162\n",
      "Iteration 939, loss = 342722493834.20843506\n",
      "Iteration 940, loss = 343269728213.54156494\n",
      "Iteration 941, loss = 343131785249.63128662\n",
      "Iteration 942, loss = 342387764392.47869873\n",
      "Iteration 943, loss = 342800170827.13806152\n",
      "Iteration 944, loss = 343120544374.27264404\n",
      "Iteration 945, loss = 342824640762.82611084\n",
      "Iteration 946, loss = 342979333352.97973633\n",
      "Iteration 947, loss = 343063558351.90643311\n",
      "Iteration 948, loss = 342292150382.54113770\n",
      "Iteration 949, loss = 342882247073.36950684\n",
      "Iteration 950, loss = 342542958001.37353516\n",
      "Iteration 951, loss = 342654921105.92083740\n",
      "Iteration 952, loss = 342606757412.22692871\n",
      "Iteration 953, loss = 342264895636.62750244\n",
      "Iteration 954, loss = 342369482294.17443848\n",
      "Iteration 955, loss = 342085162835.73205566\n",
      "Iteration 956, loss = 342320239257.19873047\n",
      "Iteration 957, loss = 342144268793.39794922\n",
      "Iteration 958, loss = 342362836385.52215576\n",
      "Iteration 959, loss = 341663174637.48406982\n",
      "Iteration 960, loss = 341724260352.73852539\n",
      "Iteration 961, loss = 341760501975.77178955\n",
      "Iteration 962, loss = 342000787223.97003174\n",
      "Iteration 963, loss = 341819072740.89990234\n",
      "Iteration 964, loss = 341526643196.41064453\n",
      "Iteration 965, loss = 341944398840.93481445\n",
      "Iteration 966, loss = 341336243443.43145752\n",
      "Iteration 967, loss = 341592610034.39526367\n",
      "Iteration 968, loss = 341614838923.68597412\n",
      "Iteration 969, loss = 341371245842.77770996\n",
      "Iteration 970, loss = 341094127729.93750000\n",
      "Iteration 971, loss = 341594033648.36785889\n",
      "Iteration 972, loss = 341637073284.76446533\n",
      "Iteration 973, loss = 341372482210.51818848\n",
      "Iteration 974, loss = 340821316515.54565430\n",
      "Iteration 975, loss = 341163441852.00891113\n",
      "Iteration 976, loss = 341442321762.75341797\n",
      "Iteration 977, loss = 340781703669.88104248\n",
      "Iteration 978, loss = 340732463489.87634277\n",
      "Iteration 979, loss = 340645132422.58502197\n",
      "Iteration 980, loss = 340859646039.95404053\n",
      "Iteration 981, loss = 341170944388.09228516\n",
      "Iteration 982, loss = 340680685248.29272461\n",
      "Iteration 983, loss = 341410131368.67840576\n",
      "Iteration 984, loss = 340844193890.17382812\n",
      "Iteration 985, loss = 340482569300.81695557\n",
      "Iteration 986, loss = 340370955606.69586182\n",
      "Iteration 987, loss = 340296597660.39471436\n",
      "Iteration 988, loss = 340501631410.38470459\n",
      "Iteration 989, loss = 340729963363.77490234\n",
      "Iteration 990, loss = 340158268500.29510498\n",
      "Iteration 991, loss = 340663468132.79815674\n",
      "Iteration 992, loss = 340282808597.07415771\n",
      "Iteration 993, loss = 340173254039.32055664\n",
      "Iteration 994, loss = 340921883578.07720947\n",
      "Iteration 995, loss = 340107958358.21563721\n",
      "Iteration 996, loss = 339735457641.30920410\n",
      "Iteration 997, loss = 340216065315.39617920\n",
      "Iteration 998, loss = 339947153128.05499268\n",
      "Iteration 999, loss = 340054509571.04486084\n",
      "Iteration 1000, loss = 339430402507.72778320\n",
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes=(512,),max_iter=iters,learning_rate_init=rate,activation='relu',solver='adam',verbose=True,batch_size=32)\n",
    "history = model.fit(X_train,y_train)\n",
    "print(len(history.loss_curve_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19c0cdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.scatter(range(1,iters+1), history.loss_curve_, alpha=0.5)\n",
    "plt.plot(history.loss_curve_, alpha=0.5)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Error Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84f21a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 3935134033.6380\n",
      "Test MSE: 2678685.2817\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "mse_val = evaluate_model(model, X_val, y_val)\n",
    "mse_test = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "print(f'Validation MSE: {mse_val**0.5:.4f}')\n",
    "print(f'Test MSE: {mse_test**0.5:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb9d90ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 2333353.3939\n"
     ]
    }
   ],
   "source": [
    "mse_train = evaluate_model(model, X_train, y_train)\n",
    "print(f'Training MSE: {mse_train**0.5:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc2e8c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2408000]\n",
      " [ 1750000]\n",
      " [ 5810000]\n",
      " [ 1750000]\n",
      " [ 5950000]\n",
      " [ 2520000]\n",
      " [ 4235000]\n",
      " [ 2450000]\n",
      " [ 8890000]\n",
      " [ 3255000]\n",
      " [ 2870000]\n",
      " [ 8190000]\n",
      " [ 2660000]\n",
      " [ 6685000]\n",
      " [ 4193000]\n",
      " [ 6790000]\n",
      " [ 5250000]\n",
      " [ 3773000]\n",
      " [ 4403000]\n",
      " [ 2660000]\n",
      " [ 6650000]\n",
      " [ 2940000]\n",
      " [ 3360000]\n",
      " [ 3353000]\n",
      " [ 3500000]\n",
      " [ 4795000]\n",
      " [ 6230000]\n",
      " [ 9800000]\n",
      " [ 3500000]\n",
      " [ 3360000]\n",
      " [ 4200000]\n",
      " [ 6860000]\n",
      " [ 3850000]\n",
      " [ 1890000]\n",
      " [ 3710000]\n",
      " [ 6440000]\n",
      " [ 4480000]\n",
      " [ 6650000]\n",
      " [ 6650000]\n",
      " [ 4270000]\n",
      " [ 3080000]\n",
      " [ 3570000]\n",
      " [ 2520000]\n",
      " [ 5145000]\n",
      " [ 5950000]\n",
      " [ 4900000]\n",
      " [ 6195000]\n",
      " [ 1890000]\n",
      " [ 5250000]\n",
      " [ 5110000]\n",
      " [ 5740000]\n",
      " [ 2660000]\n",
      " [ 6510000]\n",
      " [ 2660000]\n",
      " [ 5250000]\n",
      " [ 5110000]\n",
      " [ 4690000]\n",
      " [ 4620000]\n",
      " [ 3500000]\n",
      " [ 3675000]\n",
      " [ 2800000]\n",
      " [ 3675000]\n",
      " [ 6650000]\n",
      " [ 3087000]\n",
      " [ 7525000]\n",
      " [ 4550000]\n",
      " [10150000]\n",
      " [ 5810000]\n",
      " [ 3920000]\n",
      " [ 4060000]\n",
      " [ 3640000]\n",
      " [ 3640000]\n",
      " [ 4550000]\n",
      " [ 3150000]\n",
      " [ 8855000]\n",
      " [ 9800000]\n",
      " [ 5495000]\n",
      " [ 4620000]\n",
      " [12250000]\n",
      " [ 5250000]\n",
      " [ 3703000]\n",
      " [ 4165000]]\n",
      "[ 2893439.48056556  3257831.28875989  4961604.97433082  3177670.89946694\n",
      "  4377809.44874991  3627767.30111509  4368596.44242425  2869321.53630904\n",
      "  6778513.91816952  2912351.79993024  3615524.02552312  6946066.96885564\n",
      "  4702522.83912852  6662503.68540611  5757422.92587244  6421874.17411275\n",
      "  7392273.88009612  3936264.78537297  3789308.19378875  3236546.60504434\n",
      "  5325260.98364056  4263868.23895048  3264319.50389936  3172481.35814999\n",
      "  3300182.95532986  7421060.88207281  4636780.00715066  7550413.9215648\n",
      "  3225534.3530109   4614433.1396889   3923911.18613677  4423537.61391775\n",
      "  4563126.38833917  2922897.5663664   3290835.27049224  8698637.48156863\n",
      "  3985243.35287411  7408494.85053186  6758283.81944303  3788481.08886577\n",
      "  4357268.51349986  4133548.01271097  2701712.80990121  3607132.92427627\n",
      "  6839741.53763668  5069422.20862007  6481067.77925704  2962417.42779869\n",
      "  6553181.27829593  3492697.16040419  6894982.84071274  2999365.13226031\n",
      "  5259325.80244086  3037894.06253818  5270142.06291781  4652297.34609413\n",
      "  3908414.17923674  5018909.86185523  2945538.28703489  3252859.83612076\n",
      "  3811664.93736062  3193035.03969725  6293167.96298516  3892155.24926303\n",
      "  7540433.55110237  6642856.17974184 11333672.6161693   5033773.72431299\n",
      "  4688325.15327943  4238558.27136781  3845609.46317112  3884722.86841077\n",
      "  4212277.77615687  3764166.49231744  7210445.48147777  8390358.4400306\n",
      "  4526609.73778337  4098815.58801375  7551086.47330728  4561437.96745807\n",
      "  3901005.45920879  4002451.53930706]\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(y_test)\n",
    "print(y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
